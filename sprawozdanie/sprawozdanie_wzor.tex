\documentclass{classrep}
\usepackage[utf8]{inputenc}
\frenchspacing

\usepackage{graphicx}
\usepackage[usenames,dvipsnames]{color}
\usepackage[hidelinks]{hyperref}
\usepackage[section]{placeins}
\usepackage{float}
\usepackage{amsmath, amssymb, mathtools}

\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{0pt}

\usepackage{fancyhdr, lastpage}
\pagestyle{fancyplain}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\cfoot{\thepage\ / \pageref*{LastPage}}


\studycycle{Informatyka, studia dzienne, I st.}
\coursesemester{IV}

\coursename{Inteligentna Analiza Danych}
\courseyear{2017/2018}

\courseteacher{mgr inż. Paweł Tarasiuk}
\coursegroup{piątek, 10:15}

\author{%
  \studentinfo[215145@edu.p.lodz.pl]{Michał Chudzik}{215145}\\
  \studentinfo[210320@edu.p.lodz.pl]{Michał Sobczyk}{210320}%
}

\title{Zadanie 1}

\begin{document}
\maketitle
\thispagestyle{fancyplain}

\section{Cel}
	Celem zadania jest implementacja i zbadania sieci neuronowej. Implementacja składa się ze skalowalnej sieci neuronowej wykorzystującej wsteczną propagację błędów jako metodę nauki oraz momentum.

\section{Wprowadzenie}
	\textbf{Perceptron wielowarstwowy}-  głównym elementem są neurony przetwarzające. Każdy neuron posiada dowolną liczbę wejść oraz jedno wyjście. Neurony są pogrupowane w warstwy, gdzie każde dwie sąsiednie warstwy są połączone iloczynem kartezjańskim. Na wejście neuronu trafiają wyjścia wszystkich neuronów z warstwy poprzedniej.


Przetwarzanie wymaga obliczonych wcześniej wartości wyjściowych z poprzednich warstw. Ten proces dany jest poniższym wzorem
\begin{equation} \label{eq:feedforward}
	a_{t+1} = \sigma ( Wa_t + b),
\end{equation}
gdzie $a_{t+1}$ jest macierzą wyjścia warstwy neuronowej, $a_t$ macierzą wejścia warstwy neuronowej, W macierzą wag, b macierzą biasów a $\sigma$ jest funkcją aktywacji daną wzorem
\begin{equation} \label{eq:activation}
	\sigma = \frac{1}{1 + e^{-x}}.
\end{equation}


Proces nauki jest wykonany za pomocą metody najmniejszych spadków. Aktualizacja wag dana jest wzorem
\begin{equation} \label{eq:generalBackPropagationWeight}
	w_i = w_i - \lambda \cdot \frac{\delta E_{total}}{\delta w_i},
\end{equation}
gdzie $w_i$ jest wagą która jest aktualizowana, $\lambda$ współczynnik uczenia, $E_{total}$ dany jest wzorem
\begin{equation} \label{eq:totalError}
	E_{total} = \sum \frac{1}{2}(target - output)^2,
\end{equation}
gdzie output oznacza wynik końcowy sieci neuronowej a target oczekiwaną wartość tegoż wyniku.


Aktualizacja biasów zachodzi w sposób
\begin{equation} \label{eq:generalBackPropagationBias}
	b_i = b_i - \lambda \cdot \frac{\delta E_{total}}{\delta b_i},
\end{equation}
gdzie $b_i$ jest zmienianym biasem.

\section{Materiały i metody}

\section{Wyniki}
			

\section{Dyskusja}

\section{Wnioski}


\begin{thebibliography}{0}
\bibitem{irisdataset}https://archive.ics.uci.edu/ml/datasets/iris
Zbiór danych dla irysów
\bibitem{backPopgation}https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/
Back Propagation
\end{thebibliography}

\end{document}